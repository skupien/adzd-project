{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 07:37:12,991\tINFO worker.py:1529 -- Started a local Ray instance. View the dashboard at \u001B[1m\u001B[32m127.0.0.1:8265 \u001B[39m\u001B[22m\n",
      "2022-12-16 07:37:26,626\tWARNING read_api.py:326 -- ⚠️  The number of blocks in this dataset (1) limits its parallelism to 1 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dataset]: Run `pip install tqdm` to enable progress reporting.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "# Load data.\n",
    "dataset = ray.data.read_csv(\"s3://anonymous@air-example-data/breast_cancer.csv\")\n",
    "\n",
    "# Split data into train and validation.\n",
    "train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)\n",
    "\n",
    "# Create a test dataset by dropping the target column.\n",
    "test_dataset = valid_dataset.drop_columns(cols=[\"target\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from ray.data.preprocessors import Concatenator, Chain, StandardScaler\n",
    "\n",
    "# Create a preprocessor to scale some columns and concatenate the result.\n",
    "preprocessor = Chain(\n",
    "    StandardScaler(columns=[\"mean radius\", \"mean texture\"]),\n",
    "    Concatenator(exclude=[\"target\"], dtype=np.float32),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(RayTrainWorker pid=80174)\u001B[0m 2022-12-16 07:38:54,663\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "\u001B[2m\u001B[36m(RayTrainWorker pid=80174)\u001B[0m [W ProcessGroupGloo.cpp:724] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "\u001B[2m\u001B[36m(RayTrainWorker pid=80175)\u001B[0m [W ProcessGroupGloo.cpp:724] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "\u001B[2m\u001B[36m(RayTrainWorker pid=80176)\u001B[0m [W ProcessGroupGloo.cpp:724] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "\u001B[2m\u001B[36m(RayTrainWorker pid=80174)\u001B[0m 2022-12-16 07:38:55,972\tINFO train_loop_utils.py:270 -- Moving model to device: cpu\n",
      "\u001B[2m\u001B[36m(RayTrainWorker pid=80174)\u001B[0m 2022-12-16 07:38:55,972\tINFO train_loop_utils.py:330 -- Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<div class=\"trialProgress\">\n  <h3>Trial Progress</h3>\n  <table>\n<thead>\n<tr><th>Trial name              </th><th style=\"text-align: right;\">  _time_this_iter_s</th><th style=\"text-align: right;\">  _timestamp</th><th style=\"text-align: right;\">  _training_iteration</th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th>hostname  </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">   loss</th><th>node_ip  </th><th style=\"text-align: right;\">  pid</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n</thead>\n<tbody>\n<tr><td>TorchTrainer_44c17_00000</td><td style=\"text-align: right;\">          0.0964417</td><td style=\"text-align: right;\">  1671172736</td><td style=\"text-align: right;\">                    1</td><td>2022-12-16_07-38-56</td><td>False </td><td>                </td><td>6305d946512447fa828a7c994344995a</td><td>krk-mp5ek </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">4.79171</td><td>127.0.0.1</td><td style=\"text-align: right;\">80144</td><td>True               </td><td style=\"text-align: right;\">             13.8375</td><td style=\"text-align: right;\">           13.8375</td><td style=\"text-align: right;\">       13.8375</td><td style=\"text-align: right;\"> 1671172736</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>44c17_00000</td><td style=\"text-align: right;\">    0.0380981</td></tr>\n</tbody>\n</table>\n</div>\n<style>\n.trialProgress {\n  display: flex;\n  flex-direction: column;\n  color: var(--jp-ui-font-color1);\n}\n.trialProgress h3 {\n  font-weight: bold;\n}\n.trialProgress td {\n  white-space: nowrap;\n}\n</style>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 07:39:00,876\tINFO tune.py:762 -- Total run time: 24.38 seconds (24.22 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last result: {'loss': 0.6072853803634644, '_timestamp': 1671172738, '_time_this_iter_s': 0.10623908042907715, '_training_iteration': 20, 'time_this_iter_s': 0.10453581809997559, 'should_checkpoint': True, 'done': True, 'timesteps_total': None, 'episodes_total': None, 'training_iteration': 20, 'trial_id': '44c17_00000', 'experiment_id': '6305d946512447fa828a7c994344995a', 'date': '2022-12-16_07-38-58', 'timestamp': 1671172738, 'time_total_s': 16.073720932006836, 'pid': 80144, 'hostname': 'krk-mp5ek', 'node_ip': '127.0.0.1', 'config': {}, 'time_since_restore': 16.073720932006836, 'timesteps_since_restore': 0, 'iterations_since_restore': 20, 'warmup_time': 0.03809809684753418, 'experiment_tag': '0'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from ray import train\n",
    "from ray.air import session\n",
    "from ray.air.config import ScalingConfig\n",
    "from ray.train.torch import TorchCheckpoint, TorchTrainer\n",
    "\n",
    "\n",
    "def create_model(input_features):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_features=input_features, out_features=16),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16, 16),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16, 1),\n",
    "        nn.Sigmoid(),\n",
    "    )\n",
    "\n",
    "\n",
    "def train_loop_per_worker(config):\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    lr = config[\"lr\"]\n",
    "    epochs = config[\"num_epochs\"]\n",
    "    num_features = config[\"num_features\"]\n",
    "\n",
    "    # Get the Ray Dataset shard for this data parallel worker,\n",
    "    # and convert it to a PyTorch Dataset.\n",
    "    train_data = session.get_dataset_shard(\"train\")\n",
    "    # Create model.\n",
    "    model = create_model(num_features)\n",
    "    model = train.torch.prepare_model(model)\n",
    "\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    for cur_epoch in range(epochs):\n",
    "        for batch in train_data.iter_torch_batches(\n",
    "            batch_size=batch_size, dtypes=torch.float32\n",
    "        ):\n",
    "            # \"concat_out\" is the output column of the Concatenator.\n",
    "            inputs, labels = batch[\"concat_out\"], batch[\"target\"]\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(inputs)\n",
    "            train_loss = loss_fn(predictions, labels.unsqueeze(1))\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "        loss = train_loss.item()\n",
    "        session.report({\"loss\": loss}, checkpoint=TorchCheckpoint.from_model(model))\n",
    "\n",
    "\n",
    "num_features = len(train_dataset.schema().names) - 1\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config={\n",
    "        \"batch_size\": 128,\n",
    "        \"num_epochs\": 20,\n",
    "        \"num_features\": num_features,\n",
    "        \"lr\": 0.001,\n",
    "    },\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=3,  # Number of workers to use for data parallelism.\n",
    "        use_gpu=False,\n",
    "        trainer_resources={\"CPU\": 0},  # so that the example works on Colab.\n",
    "    ),\n",
    "    datasets={\"train\": train_dataset},\n",
    "    preprocessor=preprocessor,\n",
    ")\n",
    "# Execute training.\n",
    "result = trainer.fit()\n",
    "print(f\"Last result: {result.metrics}\")\n",
    "# Last result: {'loss': 0.6559339960416158, ...}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "param_space = {\"train_loop_config\": {\"lr\": tune.loguniform(0.0001, 0.01)}}\n",
    "metric = \"loss\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/84/dgy397h5257b3nf_mttpl2xr0000gq/T/ipykernel_79908/3190221440.py:4: UserWarning: Executing `.fit()` may leave less than 20% of CPUs in this cluster for Dataset execution, which can lead to resource contention or hangs. To avoid this, reserve at least 20% of node CPUs for Dataset execution by setting `_max_cpu_fraction_per_node = 0.8` in the Trainer scaling_config. See https://docs.ray.io/en/master/data/dataset-internals.html#datasets-and-tune for more info.\n",
      "  tuner = Tuner(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[1m\u001B[36m(scheduler +2m30s)\u001B[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "\u001B[2m\u001B[1m\u001B[33m(scheduler +2m30s)\u001B[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001B[2m\u001B[1m\u001B[33m(scheduler +3m5s)\u001B[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001B[2m\u001B[1m\u001B[33m(scheduler +3m40s)\u001B[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001B[2m\u001B[1m\u001B[33m(scheduler +4m15s)\u001B[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001B[2m\u001B[1m\u001B[33m(scheduler +4m50s)\u001B[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001B[2m\u001B[1m\u001B[33m(scheduler +5m25s)\u001B[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001B[2m\u001B[1m\u001B[33m(scheduler +6m0s)\u001B[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001B[2m\u001B[1m\u001B[33m(scheduler +6m35s)\u001B[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001B[2m\u001B[1m\u001B[33m(scheduler +7m11s)\u001B[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001B[2m\u001B[1m\u001B[33m(scheduler +7m46s)\u001B[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001B[2m\u001B[1m\u001B[33m(scheduler +8m21s)\u001B[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001B[2m\u001B[1m\u001B[33m(scheduler +8m56s)\u001B[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001B[2m\u001B[1m\u001B[33m(scheduler +9m31s)\u001B[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001B[2m\u001B[1m\u001B[33m(scheduler +10m6s)\u001B[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 07:47:16,098\tWARNING tune.py:690 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2022-12-16 07:47:18,283\tWARNING worker.py:1851 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 0814ec4a47beea5496b8800cbed13ed229cc0e9001000000 Worker ID: 81f9d07db250e0fd88483df8c679e58eba2e557ab43dbd948ba35d5e Node ID: c7926c38fee94b16f6e3d371bae94f8b9ab4889340b4f7e3fd0c96fe Worker IP address: 127.0.0.1 Worker port: 56709 Worker PID: 80160 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "2022-12-16 07:47:18,363\tERROR tune.py:758 -- Trials did not complete: [TorchTrainer_5356d_00000, TorchTrainer_5356d_00001, TorchTrainer_5356d_00002, TorchTrainer_5356d_00003, TorchTrainer_5356d_00004]\n",
      "2022-12-16 07:47:18,365\tINFO tune.py:762 -- Total run time: 497.41 seconds (497.15 seconds for the tuning loop).\n",
      "2022-12-16 07:47:18,366\tWARNING tune.py:768 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n",
      "2022-12-16 07:47:18,374\tWARNING experiment_analysis.py:627 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No best trial found for the given metric: loss. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 13\u001B[0m\n\u001B[1;32m     10\u001B[0m result_grid \u001B[38;5;241m=\u001B[39m tuner\u001B[38;5;241m.\u001B[39mfit()\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Fetch the best result.\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m best_result \u001B[38;5;241m=\u001B[39m \u001B[43mresult_grid\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_best_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest Result:\u001B[39m\u001B[38;5;124m\"\u001B[39m, best_result)\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Best Result: Result(metrics={'loss': 0.278409322102863, ...})\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/adzd-project/venv/lib/python3.9/site-packages/ray/tune/result_grid.py:134\u001B[0m, in \u001B[0;36mResultGrid.get_best_result\u001B[0;34m(self, metric, mode, scope, filter_nan_and_inf)\u001B[0m\n\u001B[1;32m    123\u001B[0m     error_msg \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    124\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo best trial found for the given metric: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    125\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetric \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_experiment_analysis\u001B[38;5;241m.\u001B[39mdefault_metric\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    126\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis means that no trial has reported this metric\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    127\u001B[0m     )\n\u001B[1;32m    128\u001B[0m     error_msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    129\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, or all values reported for this metric are NaN. To not ignore NaN \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    130\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalues, you can set the `filter_nan_and_inf` arg to False.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    131\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m filter_nan_and_inf\n\u001B[1;32m    132\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    133\u001B[0m     )\n\u001B[0;32m--> 134\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(error_msg)\n\u001B[1;32m    136\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_trial_to_result(best_trial)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: No best trial found for the given metric: loss. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False."
     ]
    }
   ],
   "source": [
    "from ray.tune.tuner import Tuner, TuneConfig\n",
    "from ray.air.config import RunConfig\n",
    "\n",
    "tuner = Tuner(\n",
    "    trainer,\n",
    "    param_space=param_space,\n",
    "    tune_config=TuneConfig(num_samples=5, metric=metric, mode=\"min\"),\n",
    ")\n",
    "# Execute tuning.\n",
    "result_grid = tuner.fit()\n",
    "\n",
    "# Fetch the best result.\n",
    "best_result = result_grid.get_best_result()\n",
    "print(\"Best Result:\", best_result)\n",
    "# Best Result: Result(metrics={'loss': 0.278409322102863, ...})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from ray.train.torch import TorchPredictor\n",
    "\n",
    "# You can also create a checkpoint from a trained model using\n",
    "# `TorchCheckpoint.from_model`.\n",
    "checkpoint = best_result.checkpoint\n",
    "\n",
    "batch_predictor = BatchPredictor.from_checkpoint(\n",
    "    checkpoint, TorchPredictor, model=create_model(num_features)\n",
    ")\n",
    "\n",
    "predicted_probabilities = batch_predictor.predict(test_dataset)\n",
    "predicted_probabilities.show()\n",
    "# {'predictions': array([1.], dtype=float32)}\n",
    "# {'predictions': array([0.], dtype=float32)}\n",
    "# ..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
